{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01093a9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced Time Series Forecasting with Deep Learning: LSTM with Attention\n",
    "# This notebook implements the full project:\n",
    "# - Load/generate a univariate time series\n",
    "# - Preprocess & create lookback windows\n",
    "# - Baseline LSTM model\n",
    "# - Attention-based LSTM model\n",
    "# - Train, evaluate, and compare using RMSE/MAE\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models, callbacks, optimizers\n",
    "\n",
    "from statsmodels.datasets import airpassengers\n",
    "\n",
    "print('TensorFlow version:', tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c452e54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load AirPassengers dataset (monthly airline passengers)\n",
    "data = airpassengers.load_pandas().data\n",
    "display(data.head())\n",
    "display(data.tail())\n",
    "print('Length of series:', len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8a376aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare univariate time series\n",
    "ts = data['value'].astype('float32').values.reshape(-1, 1)\n",
    "\n",
    "# Create a monthly DatetimeIndex for plotting\n",
    "time_index = pd.date_range(start='1949-01', periods=len(ts), freq='M')\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(time_index, ts)\n",
    "plt.title('AirPassengers Time Series')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Passengers')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25ab4522",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaling the data\n",
    "scaler = MinMaxScaler()\n",
    "ts_scaled = scaler.fit_transform(ts)\n",
    "\n",
    "def create_sequences(series, lookback=24):\n",
    "    \"\"\"Convert a 1D scaled series to sequences for LSTM.\n",
    "    \n",
    "    series: shape (N, 1)\n",
    "    returns X: (N-lookback, lookback, 1), y: (N-lookback, 1)\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(series) - lookback):\n",
    "        X.append(series[i:i+lookback])\n",
    "        y.append(series[i+lookback])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# You can adjust LOOKBACK as a hyperparameter\n",
    "LOOKBACK = 24  # 24 months (2 years)\n",
    "X_all, y_all = create_sequences(ts_scaled, lookback=LOOKBACK)\n",
    "\n",
    "print('X_all shape:', X_all.shape)\n",
    "print('y_all shape:', y_all.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a71e8ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train / Val / Test split\n",
    "n_total = len(X_all)\n",
    "n_train = int(0.7 * n_total)\n",
    "n_val = int(0.15 * n_total)\n",
    "\n",
    "X_train, y_train = X_all[:n_train], y_all[:n_train]\n",
    "X_val, y_val = X_all[n_train:n_train+n_val], y_all[n_train:n_train+n_val]\n",
    "X_test, y_test = X_all[n_train+n_val:], y_all[n_train+n_val:]\n",
    "\n",
    "print('Train shape:', X_train.shape, y_train.shape)\n",
    "print('Val shape   :', X_val.shape, y_val.shape)\n",
    "print('Test shape  :', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a958fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper to compute RMSE and MAE in original scale\n",
    "def compute_metrics(y_true_scaled, y_pred_scaled, scaler):\n",
    "    y_true = scaler.inverse_transform(y_true_scaled)\n",
    "    y_pred = scaler.inverse_transform(y_pred_scaled)\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    mae = mean_absolute_error(y_true, y_pred)\n",
    "    return rmse, mae"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e74d3ec4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Baseline LSTM model\n",
    "def build_baseline_lstm(units=64, dropout=0.2, lr=1e-3):\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=(LOOKBACK, 1)),\n",
    "        layers.LSTM(units, return_sequences=False),\n",
    "        layers.Dropout(dropout),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    opt = optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(loss='mse', optimizer=opt)\n",
    "    return model\n",
    "\n",
    "baseline_model = build_baseline_lstm()\n",
    "baseline_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6e6ff30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train baseline LSTM\n",
    "es = callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "history_base = baseline_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[es],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ebf7d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate baseline on test set\n",
    "y_test_pred_base = baseline_model.predict(X_test)\n",
    "\n",
    "rmse_base, mae_base = compute_metrics(y_test, y_test_pred_base, scaler)\n",
    "print(f'Baseline LSTM - RMSE: {rmse_base:.3f}, MAE: {mae_base:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86da293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot baseline predictions vs actual\n",
    "y_true_inv = scaler.inverse_transform(y_test)\n",
    "y_pred_inv = scaler.inverse_transform(y_test_pred_base)\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(y_true_inv, label='Actual')\n",
    "plt.plot(y_pred_inv, label='Baseline LSTM Pred')\n",
    "plt.title('Baseline LSTM - Test Set')\n",
    "plt.xlabel('Time index')\n",
    "plt.ylabel('Passengers')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f58febf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Bahdanau-style Attention layer\n",
    "class BahdanauAttention(layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super().__init__()\n",
    "        self.W1 = layers.Dense(units)\n",
    "        self.V = layers.Dense(1)\n",
    "\n",
    "    def call(self, values):\n",
    "        \"\"\"values: LSTM outputs for all time steps, shape (batch, timesteps, hidden)\"\"\"\n",
    "        score = self.V(tf.nn.tanh(self.W1(values)))  # (batch, timesteps, 1)\n",
    "        attention_weights = tf.nn.softmax(score, axis=1)  # softmax over time\n",
    "        context_vector = attention_weights * values\n",
    "        context_vector = tf.reduce_sum(context_vector, axis=1)  # sum over time\n",
    "        return context_vector, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b925ca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention-based LSTM model (stacked LSTM + attention)\n",
    "def build_attention_lstm(units=64, att_units=32, dropout=0.2, lr=1e-3):\n",
    "    inputs = layers.Input(shape=(LOOKBACK, 1))\n",
    "    x = layers.LSTM(units, return_sequences=True)(inputs)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    x2 = layers.LSTM(units, return_sequences=True)(x)  # stacked LSTM\n",
    "\n",
    "    attention_layer = BahdanauAttention(att_units)\n",
    "    context_vector, att_weights = attention_layer(x2)\n",
    "\n",
    "    x_out = layers.Dropout(dropout)(context_vector)\n",
    "    outputs = layers.Dense(1)(x_out)\n",
    "\n",
    "    model = models.Model(inputs=inputs, outputs=outputs)\n",
    "    opt = optimizers.Adam(learning_rate=lr)\n",
    "    model.compile(loss='mse', optimizer=opt)\n",
    "    return model\n",
    "\n",
    "att_model = build_attention_lstm()\n",
    "att_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a85a663",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Attention LSTM\n",
    "es2 = callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "\n",
    "history_att = att_model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[es2],\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1935da96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate Attention LSTM on test set\n",
    "y_test_pred_att = att_model.predict(X_test)\n",
    "\n",
    "rmse_att, mae_att = compute_metrics(y_test, y_test_pred_att, scaler)\n",
    "print(f'Attention LSTM - RMSE: {rmse_att:.3f}, MAE: {mae_att:.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac05b12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot Attention predictions vs actual\n",
    "y_pred_inv_att = scaler.inverse_transform(y_test_pred_att)\n",
    "\n",
    "plt.figure(figsize=(8, 3))\n",
    "plt.plot(y_true_inv, label='Actual')\n",
    "plt.plot(y_pred_inv_att, label='Attention LSTM Pred')\n",
    "plt.title('Attention LSTM - Test Set')\n",
    "plt.xlabel('Time index')\n",
    "plt.ylabel('Passengers')\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624a3187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple hyperparameter experiment for baseline LSTM (units)\n",
    "def train_and_eval_baseline(units):\n",
    "    model = build_baseline_lstm(units=units)\n",
    "    es_local = callbacks.EarlyStopping(monitor='val_loss', patience=7, restore_best_weights=True)\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        validation_data=(X_val, y_val),\n",
    "        epochs=60,\n",
    "        batch_size=32,\n",
    "        callbacks=[es_local],\n",
    "        verbose=0\n",
    "    )\n",
    "    preds = model.predict(X_val)\n",
    "    rmse, mae = compute_metrics(y_val, preds, scaler)\n",
    "    return rmse, mae\n",
    "\n",
    "units_list = [32, 64, 128]\n",
    "results = []\n",
    "for u in units_list:\n",
    "    rmse_u, mae_u = train_and_eval_baseline(u)\n",
    "    results.append((u, rmse_u, mae_u))\n",
    "    print(f'Baseline LSTM (units={u}) - RMSE: {rmse_u:.3f}, MAE: {mae_u:.3f}')\n",
    "\n",
    "results"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
